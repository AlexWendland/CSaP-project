{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#ALWAYS RUN FIRST!\n",
    "\n",
    "#Import libraries and packages. Don't worry about the warning if running it on windows, so far not hit an issue. (yn)\n",
    "\n",
    "import re\n",
    "from pylab import *\n",
    "import csv\n",
    "import psycopg2\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import gensim\n",
    "import os\n",
    "\n",
    "#Set the names of the files in which you want to save data.\n",
    "\n",
    "profile_data_name = 'profile_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to access CRM CSaP database ...\n",
      "... data downloaded and saved to disk.\n"
     ]
    }
   ],
   "source": [
    "#Extract the data from the CRM. Don't run if you have an up to date copy of the CRM.\n",
    "\n",
    "try:\n",
    "    \n",
    "    #Opens connection to the CRM asks for peoples name and description.\n",
    "    #Output rows for row in rows row[0] - first name, row[1] - second name, row[2] - description.\n",
    "    \n",
    "    print(\"Trying to access CRM CSaP database ...\")\n",
    "    \n",
    "    conn = psycopg2.connect(SERVER_INFO)\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\"\"\"SELECT\n",
    "    person.first_name,\n",
    "    person.last_name,\n",
    "    person.description\n",
    "    FROM people_person as person\n",
    "    ;\n",
    "    \"\"\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    #Saves data to the file called above.\n",
    "    \n",
    "    with open(os.getcwd() + '\\data\\\\' + profile_data_name + '.csv','w+',newline ='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        for row in rows:\n",
    "            wr.writerow([r.encode('utf-8') for r in row])\n",
    "    \n",
    "    print(\"... data downloaded and saved to disk.\")\n",
    "    \n",
    "except:\n",
    "    \n",
    "    #If server isn't online this collects data from the save file.\n",
    "    \n",
    "    print(\"... can't access server, is the tunnel set up? Can continue on previously saved data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for cleaning the text and getting it ready to be procced.\n",
    "\n",
    "#Gets rid of HTML tags and end of line markers.\n",
    "#Input: String of text from CRM or internet.\n",
    "#Output: Cleaned up string of text without HTML tags or end of line markers.\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    #Removes HTML tags.\n",
    "    \n",
    "    clean = re.compile('<.*?>')\n",
    "    temp_text = re.sub(clean, '', text)\n",
    "    \n",
    "    #Removes rouge utf-8 code.\n",
    "    \n",
    "    clean = re.compile('\\\\\\\\x\\w\\w')\n",
    "    temp_text = re.sub(clean, '', temp_text)\n",
    "    \n",
    "    clean = re.compile('\\\\\\\\x\\w')\n",
    "    temp_text = re.sub(clean, '', temp_text)\n",
    "    \n",
    "    #Removes end of line indicators and other junk.\n",
    "    \n",
    "    tags = ['\\\\r','\\\\n','/','\\\\t','\\\\']\n",
    "    \n",
    "    for tag in tags:\n",
    "        temp_text = temp_text.replace(tag,'')\n",
    "    \n",
    "    return temp_text\n",
    "\n",
    "#Tokenizes text, seperates it into a string of words and grammar.\n",
    "#Input: A string of text.\n",
    "#Output: A list of words and grammar in order all in lower case.\n",
    "\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "#Lemmatiser, this finds the root word (i.e. depluralises).\n",
    "#Input: a token, i.e. a single word or grammar.\n",
    "#Output: a lemma which is the base of the word and association \n",
    "\n",
    "def get_lemma(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "#Checks if two words are synonyms or antonyms.\n",
    "#Input: Takes two strings word_1 and word_2, they should be prepared words.\n",
    "#Output: Returns 1 if they are synonyms, -1 if they are antonyms or 0 otherwise.\n",
    "\n",
    "def is_synonym_antonym(word_1,word_2):\n",
    "    word_1_synonyms = set([word for synset in wn.synsets(word_1) for word in synset.lemma_names()])\n",
    "    word_2_synonyms = set([word for synset in wn.synsets(word_2) for word in synset.lemma_names()])\n",
    "    word_1_antonyms = set([ant.name() for synset in wn.synsets(word_1) for syn in synset.lemmas() for ant in syn.antonyms()])\n",
    "    word_2_antonyms = set([ant.name() for synset in wn.synsets(word_2) for syn in synset.lemmas() for ant in syn.antonyms()])\n",
    "\n",
    "    if word_1_synonyms & word_2_synonyms or word_1_antonyms & word_2_antonyms:\n",
    "        return 1\n",
    "\n",
    "    if word_1_synonyms & word_2_antonyms or word_2_synonyms & word_1_antonyms:\n",
    "        return -1\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "#Prepares text for the analysis, tokenizes texts, gets rid of words length less than 4 and filters out non-useful words then\n",
    "#Lemmatisers the text.\n",
    "#Input: A string of text you want to analysis.\n",
    "#Output: A list of Lemmas of the meaningful words.\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [tok[0] for tok in nltk.pos_tag(tokens) if tok[1][0] == 'N']\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load up data ...\n",
      "... data successfully uploaded.\n"
     ]
    }
   ],
   "source": [
    "#This uploads the data into the program from the file, cleaning the data whilst it does it.\n",
    "\n",
    "print(\"Trying to load up data ...\")\n",
    "\n",
    "people = []\n",
    "\n",
    "try:\n",
    "    with open(os.getcwd() + '\\data\\\\' + profile_data_name+'.csv', 'r') as csvfile:\n",
    "        dump = list(csv.reader(csvfile))\n",
    "        for row in dump:\n",
    "            people.append([clean_text(r[2:-1]) for r in row])\n",
    "    \n",
    "    print(\"... data successfully uploaded.\")\n",
    "    \n",
    "except:\n",
    "        \n",
    "    print(\".. no back up data, please connect to server.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Currently takes the profile data, gets rid of people with insufficent data (=< 100 charracters) then prepares their profile\n",
    "#data for analysis. Sets text_data to be a list with first name, second name, prepared profile.\n",
    "\n",
    "people_prepared = []\n",
    "\n",
    "for person in people:\n",
    "    if len(person[2]) > 200: \n",
    "        people_prepared.append([person[0], person[1], prepare_text_for_lda(person[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.062*\"university\" + 0.050*\"research\" + 0.024*\"cambridge\" + 0.021*\"professor\"')\n",
      "(1, '0.067*\"health\" + 0.019*\"policy\" + 0.014*\"research\" + 0.009*\"development\"')\n",
      "(2, '0.027*\"director\" + 0.026*\"business\" + 0.017*\"management\" + 0.017*\"cambridge\"')\n",
      "(3, '0.024*\"research\" + 0.018*\"university\" + 0.013*\"cambridge\" + 0.012*\"material\"')\n",
      "(4, '0.040*\"policy\" + 0.030*\"research\" + 0.024*\"science\" + 0.019*\"university\"')\n"
     ]
    }
   ],
   "source": [
    "#Theme extractor, submit a list of texts and it will extract topic_num themes each summerised by word_num worth of words each\n",
    "#with individual ratings on how important the word is.\n",
    "#Input: List of prepared text.\n",
    "#Output: a list \n",
    "\n",
    "def get_themes(text_data, topic_num, word_num):\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "    dictionary.save('dictionary.gensim')\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = topic_num, id2word=dictionary, passes=15)\n",
    "    ldamodel.save('model5.gensim')\n",
    "    return ldamodel.print_topics(num_words=word_num)\n",
    "\n",
    "for theme in get_themes([person[2] for person in people_prepared], 5,4):\n",
    "    print(theme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ian Palmer with probability 0.18518518518518517\n",
      "Robin North with probability 0.17647058823529413\n",
      "Simon Warburton with probability 0.1724137931034483\n",
      "Ian Bamford with probability 0.16279069767441862\n",
      "Kaveh Jahanshahi with probability 0.16071428571428573\n",
      "Dan Clarke with probability 0.16\n",
      "Philippa Benfield with probability 0.14814814814814814\n",
      "Fod Barnes with probability 0.14285714285714285\n",
      "Jim Platts with probability 0.14035087719298245\n",
      "Ettore Settanni with probability 0.14\n"
     ]
    }
   ],
   "source": [
    "#Set words to be the words you want to compare against, this prepares this text into a nice list. (TO ADD THEME EXTRACTION)\n",
    "\n",
    "words = 'traditional industries reorient changing'\n",
    "\n",
    "prepared_words = [get_lemma(word) for word in tokenize(words)]\n",
    "    \n",
    "#This searches the list of people for who is relevant to the words you have asked about and orderes them by what proportion of\n",
    "#the words in there profile fit these words.\n",
    "\n",
    "relevant_people = []\n",
    "\n",
    "for person in people_prepared:\n",
    "    count = 0\n",
    "    for word in prepared_words:\n",
    "        for profile_word in person[2]:\n",
    "            count += abs(is_synonym_antonym(word,profile_word))\n",
    "    if count != 0:\n",
    "        relevant_people.append([person[0],person[1],count/len(person[2])])\n",
    "\n",
    "relevant_people = sorted(relevant_people,key=lambda person: person[2], reverse = True)\n",
    "\n",
    "#This prints the top 10 fitting people.\n",
    "\n",
    "for i in range(10):\n",
    "    print(relevant_people[i][0] + ' ' + relevant_people[i][1] + ' with probability ' + str(relevant_people[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
