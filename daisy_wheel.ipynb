{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#ALWAYS RUN FIRST!\n",
    "\n",
    "#Import libraries and packages. Don't worry about the warning if running it on windows, so far not hit an issue. (yn)\n",
    "\n",
    "import re\n",
    "from pylab import *\n",
    "import csv\n",
    "import psycopg2\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import gensim\n",
    "import os\n",
    "\n",
    "#Set the names of the files in which you want to save data, these will all be in the data file.\n",
    "\n",
    "people_data_name = 'people_data'\n",
    "department_data_name = 'department_names'\n",
    "meeting_data_name = 'daisy_wheel_meetings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to access CRM CSaP database ...\n",
      "... data downloaded and saved to disk.\n"
     ]
    }
   ],
   "source": [
    "#Extract the data from the CRM. Don't run if you have an up to date copy of the CRM.\n",
    "\n",
    "try:\n",
    "    \n",
    "    #Opens connection to the CRM asks for peoples name and description.\n",
    "    #Output rows for row in rows row[0] - first name, row[1] - second name, row[2] - description.\n",
    "    \n",
    "    print(\"Trying to access CRM CSaP database ...\")\n",
    "    \n",
    "    conn = psycopg2.connect(SERVER_INFO)\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\"\"\"SELECT\n",
    "    person.first_name,\n",
    "    person.last_name,\n",
    "    organization.name\n",
    "    FROM people_person as person\n",
    "    INNER JOIN\n",
    "    organizations_personorganizationrole\n",
    "    ON\n",
    "    organizations_personorganizationrole.person_id = person.id\n",
    "    INNER JOIN\n",
    "    organizations_organization as organization\n",
    "    ON\n",
    "    organization.id = organizations_personorganizationrole.organization_id\n",
    "    ;\n",
    "    \"\"\")\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    #Saves data to the file called above.\n",
    "    \n",
    "    with open(os.getcwd() + '\\data\\\\' + people_data_name +'.csv','w+',newline ='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        for row in rows:\n",
    "            wr.writerow([r.encode('utf-8') for r in row])\n",
    "    \n",
    "    cur.execute(\"\"\"SELECT\n",
    "    people_policyfellowshipschedule.date,\n",
    "    fellow.first_name,\n",
    "    fellow.last_name,\n",
    "    academic.first_name,\n",
    "    academic.last_name\n",
    "    FROM people_policyfellowshipschedule_people\n",
    "    JOIN people_person as academic ON people_policyfellowshipschedule_people.person_id = academic.id\n",
    "    JOIN people_policyfellowshipschedule ON people_policyfellowshipschedule_people.policyfellowshipschedule_id = people_policyfellowshipschedule.id\n",
    "    JOIN people_policyfellowship ON people_policyfellowshipschedule.policy_fellowship_id = people_policyfellowship.id\n",
    "    JOIN people_person as fellow ON people_policyfellowship.policy_fellow_id = fellow.id\n",
    "    ;\n",
    "    \"\"\")\n",
    "    \n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    #Saves data to the file called above.\n",
    "    \n",
    "    with open(os.getcwd() + '\\data\\\\' + meeting_data_name +'.csv','w+',newline ='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        for row in rows:\n",
    "            wr.writerow([str(row[0]), row[1].encode('utf-8'), row[2].encode('utf-8'), row[3].encode('utf-8'), row[4].encode('utf-8')])\n",
    "    print(\"... data downloaded and saved to disk.\")\n",
    "    \n",
    "except:\n",
    "    \n",
    "    #If server isn't online this collects data from the save file.\n",
    "    \n",
    "    print(\"... can't access server, is the tunnel set up? Can continue on previously saved data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for cleaning the text and getting it ready to be procced.\n",
    "\n",
    "#Gets rid of HTML tags and end of line markers.\n",
    "#Input: String of text from CRM or internet.\n",
    "#Output: Cleaned up string of text without HTML tags or end of line markers.\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    #Removes HTML tags.\n",
    "    \n",
    "    clean = re.compile('<.*?>')\n",
    "    temp_text = re.sub(clean, '', text)\n",
    "    \n",
    "    #Removes rouge utf-8 code.\n",
    "    \n",
    "    clean = re.compile('\\\\\\\\x\\w\\w')\n",
    "    temp_text = re.sub(clean, '', temp_text)\n",
    "    \n",
    "    clean = re.compile('\\\\\\\\x\\w')\n",
    "    temp_text = re.sub(clean, '', temp_text)\n",
    "    \n",
    "    #Removes end of line indicators and other junk.\n",
    "    \n",
    "    tags = ['\\\\r','\\\\n','/','\\\\t','\\\\']\n",
    "    \n",
    "    for tag in tags:\n",
    "        temp_text = temp_text.replace(tag,'')\n",
    "    \n",
    "    return temp_text\n",
    "\n",
    "#Tokenizes text, seperates it into a string of words and grammar.\n",
    "#Input: A string of text.\n",
    "#Output: A list of words and grammar in order all in lower case.\n",
    "\n",
    "parser = English()\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "#Lemmatiser, this finds the root word (i.e. depluralises).\n",
    "#Input: a token, i.e. a single word or grammar.\n",
    "#Output: a lemma which is the base of the word and association \n",
    "\n",
    "def get_lemma(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "    \n",
    "#Prepares text for the analysis, tokenizes texts, gets rid of words length less than 4 and filters out non-useful words then\n",
    "#Lemmatisers the text.\n",
    "#Input: A string of text you want to analysis.\n",
    "#Output: A list of Lemmas of the meaningful words.\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [tok[0] for tok in nltk.pos_tag(tokens) if tok[1][0] == 'N']\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "#Specialised version of the above function for organisations.\n",
    "#Input: A string of text you want to analysis.\n",
    "#Output: A list of Lemmas of the meaningful words.\n",
    "\n",
    "def perpare_organisation(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if (token not in en_stop)]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load up dictionary  department_names  ...\n",
      "... dictionary successfully created.\n"
     ]
    }
   ],
   "source": [
    "#This is imported as a dictionary with the names of the departments as lables and further dictionary as objects linked.\n",
    "#This uploads the dictionary data into the program from the file, cleaning the data whilst it does it.\n",
    "\n",
    "def read_dictionary_file(file_name):\n",
    "    \n",
    "    print(\"Trying to load up dictionary \", file_name, \" ...\")\n",
    "\n",
    "    new_dictionary = {}\n",
    "\n",
    "    try:\n",
    "        with open(os.getcwd() + '\\data\\\\' + file_name +'.csv', 'r') as csvfile:\n",
    "            dump = list(csv.reader(csvfile))\n",
    "            dic_place = []\n",
    "            for row in dump:\n",
    "                new_name = clean_text(row[0][2:-1])\n",
    "                cur_dic = new_dictionary\n",
    "                if new_name == 'END_DIC':\n",
    "                    del dic_place[len(dic_place) - 1]\n",
    "                else:\n",
    "                    for name in dic_place:\n",
    "                        cur_dic = cur_dic[name]\n",
    "                    cur_dic[new_name] = {}\n",
    "                    dic_place.append(new_name)\n",
    "\n",
    "        print(\"... dictionary successfully created.\")\n",
    "        \n",
    "        return new_dictionary\n",
    "\n",
    "    except :\n",
    "        \n",
    "        print(\".. no dictionary data, please connect to server.\")\n",
    "\n",
    "SFD = read_dictionary_file(department_data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load up data ...\n",
      "... data successfully uploaded.\n"
     ]
    }
   ],
   "source": [
    "#This uploads the person data into the program from the file, cleaning the data whilst it does it.\n",
    "\n",
    "print(\"Trying to load up data ...\")\n",
    "\n",
    "people = []\n",
    "meetings = []\n",
    "\n",
    "try:\n",
    "    with open(os.getcwd() + '\\data\\\\' + people_data_name +'.csv', 'r') as csvfile:\n",
    "        dump = list(csv.reader(csvfile))\n",
    "        for row in dump:\n",
    "            people.append([clean_text(r[2:-1]) for r in row])\n",
    "    \n",
    "    with open(os.getcwd() + '\\data\\\\' + meeting_data_name +'.csv', 'r') as csvfile:\n",
    "        dump = list(csv.reader(csvfile))\n",
    "        for row in dump:\n",
    "            meetings.append([datetime.strptime(row[0],'%Y-%m-%d'),clean_text(row[1][2:-1]),clean_text(row[2][2:-1]),clean_text(row[3][2:-1]),clean_text(row[4][2:-1])])\n",
    "    \n",
    "    print(\"... data successfully uploaded.\")\n",
    "    \n",
    "except:\n",
    "        \n",
    "    print(\".. no back up data, please connect to server.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorts people into 4 categories, Cambridge university people, Other university people, Other Cambirdge people and the rest.\n",
    "\n",
    "cambridge_other = []\n",
    "cambridge_university = []\n",
    "other_university = []\n",
    "other = []\n",
    "\n",
    "for person in people:\n",
    "    affiliation = tokenize(person[2])\n",
    "    if 'cambridge' in affiliation:\n",
    "        if 'uni' in affiliation or 'university' in affiliation or 'univ' in affiliation:\n",
    "            cambridge_university.append(person)\n",
    "        else:\n",
    "            cambridge_other.append(person)\n",
    "    elif 'cambridgeshire' in affiliation:\n",
    "        if 'uni' in affiliation or 'university' in affiliation or 'univ' in affiliation:\n",
    "            cambridge_university.append(person)\n",
    "        else:\n",
    "            cambridge_other.append(person)\n",
    "    else:\n",
    "        if 'uni' in affiliation or 'university' in affiliation or 'univ' in affiliation:\n",
    "            other_university.append(person)\n",
    "        else:\n",
    "            other.append(person)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks to see if the words appearing in the group name appears in the persons affilliation.\n",
    "#Input: two strings affil and group.\n",
    "#Output: Boolean based on if the inportant words from the group name appear in the persons affiliation.\n",
    "\n",
    "def is_in_group(affil,group):\n",
    "    affil_tokenized = perpare_organisation(affil)\n",
    "    for item in perpare_organisation(group):\n",
    "        if item not in affil_tokenized:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "#RECCURSIVE FUNCTION\n",
    "#Used to create a new dictionary which is a copy of the old one with the additional field people and count.\n",
    "#Input: A dictionary to copy.\n",
    "#Output: A copy of that dictionary with additional fields.\n",
    "\n",
    "def create_count(cur_dic):\n",
    "    count = {}\n",
    "    count['people'] = []\n",
    "    count['count'] = 0\n",
    "    \n",
    "    for key in cur_dic.keys():\n",
    "        count[key] = create_count(cur_dic[key])\n",
    "    \n",
    "    return count\n",
    "\n",
    "#RECCURSIVE FUNCTION\n",
    "#Used to search through out dictionary structure and find all groups which they could fit into.\n",
    "#Input: A dictionary cur_dic, the persons data and the name of the dictionaries used to get here dic_names.\n",
    "#Output: List of dictionary locations that would git the person.\n",
    "\n",
    "def sort_person(cur_dic, person, dic_names):\n",
    "    keys = cur_dic.keys()\n",
    "    possibilities = []\n",
    "    for group in keys:\n",
    "        new_list = dic_names.copy()\n",
    "        new_list.append(group)\n",
    "        if is_in_group(person[2],group):\n",
    "            possibilities.append(new_list)\n",
    "        possibilities += sort_person(cur_dic[group], person,new_list)\n",
    "    return possibilities\n",
    "\n",
    "#RECCURSIVE FUNCTION\n",
    "#Adds the person to the dictionary location specified by names.\n",
    "#Input: A dictionary, the location data of the dictionary, index of how far through you are and the person you want to add.\n",
    "#Output: Empty.\n",
    "\n",
    "def add_this(cur_dic, names, index, person):\n",
    "    if len(names) <= index:\n",
    "        cur_dic[\"people\"].append(person)\n",
    "    else:\n",
    "        add_this(cur_dic[names[index]],names, index + 1, person)\n",
    "    \n",
    "#RECCURSIVE FUNCTION\n",
    "#Sums the number of people in area of the dictionary, including lower layers.\n",
    "#Input: A dictionary to sum.\n",
    "#Output: The number of people in this dictionary\n",
    "    \n",
    "def sum_dic(cur_dic):\n",
    "    keys = [key for key in cur_dic.keys() if key not in ['people','count']]\n",
    "    for key in keys:\n",
    "        cur_dic['count'] += sum_dic(cur_dic[key])\n",
    "    cur_dic['count'] += len(cur_dic['people'])\n",
    "    return cur_dic['count']\n",
    "\n",
    "#Takes a list of people with their affiliations and sorts them according to the dictionary given.\n",
    "#Input: List of people with their affiliation in the 3rd entry of a list and a dictionary to sort them by.\n",
    "#Output: A copy of the given dictionary with additional entries at each layer containing the people that fit and the number of them.\n",
    "\n",
    "def sort_people(people, sort_dict):\n",
    "    \n",
    "    count = create_count(sort_dict)\n",
    "    \n",
    "    for person in people:\n",
    "\n",
    "        possibilities = sort_person(sort_dict, person, [])\n",
    "\n",
    "        if len(possibilities) == 0:\n",
    "            count[\"people\"].append(person)\n",
    "        elif len(possibilities) == 1:\n",
    "            add_this(count,possibilities[0],0,person)\n",
    "        else:\n",
    "            for set_1 in possibilities:\n",
    "                possibility_1 = set_1[len(set_1) - 1]\n",
    "                for set_2 in possibilities:\n",
    "                    possibility_2 = set_2[len(set_2) - 1]\n",
    "                    if possibility_1 == possibility_2:\n",
    "                        continue\n",
    "                    elif is_in_group(possibility_2,possibility_1):\n",
    "                        possibilities.remove(set_1)\n",
    "                        break\n",
    "            if len(possibilities) == 1:\n",
    "                add_this(count,possibilities[0],0,person)\n",
    "            else:\n",
    "                count[\"people\"].append(person)\n",
    "    \n",
    "    sum_dic(count)\n",
    "    \n",
    "    return count\n",
    "\n",
    "#Filters the meetins by year.\n",
    "#Input: List of all meetings, start_date and end_date.\n",
    "#Output: List of meetings occuring after start_date and before end_date inclusive.\n",
    "\n",
    "def filter_meetings(meetings, start_date, end_date):\n",
    "    new_meetings = []\n",
    "    for meeting in meetings:\n",
    "        if meeting[0] >= start_date and meeting[0] <= end_date:\n",
    "            new_meetings.append(meeting)\n",
    "    \n",
    "    return new_meetings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINT HOW MANY PEOPLE ARE IN UNACCOUNTED FOR ORGANISATIONS\n",
      "TOTAL =  1263\n",
      "Univ Cambridge 1\n",
      "Statistical Laboratory, University of Cambridge 13\n",
      "The Well-being Institute, University of Cambridge 3\n",
      "Mongolia and Inner Asia Studies Unit (MIASU), University of Cambridge 10\n",
      "Nanophotonics Centre, Cambridge University 1\n",
      "Centre of South Asian Studies, University of Cambridge 4\n",
      "Centre for Entrepreneurial Learning (CfEL), University of Cambridge 5\n",
      "Africa Together, University of Cambridge 2\n",
      "Cambridge University Entrepreneurs 1\n",
      "Cambridge University Government Policy Programme 1\n",
      "Student, Cambridge University 1\n",
      "University of Cambridge Alumni 1\n",
      "Centre for Financial Research, University of Cambridge 2\n",
      "Cambridge University Hospitals NHS Foundation Trust 29\n",
      "North West Cambridge Development, University of Cambridge 1\n",
      "UK Innovation Research Centre, University of Cambridge 1\n",
      "Gonville and Caius College, University of Cambridge 1\n",
      "University of Cambridge Computer Lab 1\n",
      "Centre for Music and Science, Cambridge University 1\n",
      "Institute of Theoretical Geophysics, University of Cambridge 1\n",
      "Cambridge University Technology and Enterprise Club (CUTEC) 8\n",
      "University of Cambridge Environment and Energy Section 1\n",
      "Cambridge University Library 8\n",
      "Institute for Manufacturing (IfM), University of Cambridge 81\n",
      "Cambridge Centre for Health Services Research, University of Cambridge 1\n",
      "Centre for Sustainable Development, University of Cambridge 16\n",
      "Centre of Development Studies, University of Cambridge 18\n",
      "Orthopaedic Research Unit, University of Cambridge 2\n",
      "University of Cambridge 783\n",
      "Centre for Music and Science, University of Cambridge 5\n",
      "CEDAR University of Cambridge 1\n",
      "Centre for Risk in the Built Environment, University of Cambridge 2\n",
      "Cambridge Language Sciences, University of Cambridge 1\n",
      "University of Cambridge student 1\n",
      "The Nanoscience Centre, University of Cambridge 3\n",
      "University of Cambridge Hispanic Society (CUHISP) 1\n",
      "Wellcome Trust-MRC Institute of Metabolic Science (IMS), University of Cambridge 7\n",
      "Centre for Science and Policy, University of Cambridge 22\n",
      "Primary Care Unit, University of Cambridge 1\n",
      "University of Cambridge Medical Library 1\n",
      "Energy@Cambridge SRI, University of Cambridge 4\n",
      "Research Centre for English and Applied Linguistics (RCEAL), University of Cambridge 1\n",
      "Conservation Science Group, University of Cambridge 1\n",
      "Strategic Partnerships Office, University of Cambridge 3\n",
      "Development Studies Committee, University of Cambridge 1\n",
      "Cambridge University Press 3\n",
      "University of Cambridge Museums 2\n",
      "Centre for Atmospheric Science, University of Cambridge 9\n",
      "Centre of Islamic Studies, University of Cambridge 1\n",
      "Centre of African Studies, University of Cambridge 4\n",
      "Cambridge University Health Partners 4\n",
      "Centre for Commonwealth Education, University of Cambridge 6\n",
      "Centre for Biomedical Science, University of Cambridge 1\n",
      "Bennett Institute for Public Policy, University of Cambridge 2\n",
      "Centre for Mathematical Sciences, University of Cambridge 7\n",
      "Uni of Cambridge 1\n",
      "University of Cambridge Botanic Garden 3\n",
      "Corporate Liaison Office (CLO), University of Cambridge 1\n",
      "Centre for Digital Built Britain, University of Cambridge 1\n",
      "Centre of Latin American Studies, University of Cambridge 2\n",
      "Sainsbury Laboratory, University of Cambridge 2\n",
      "Gurdon Institute, University of Cambridge 23\n",
      "Centre for Stem Cell Research, University of Cambridge 5\n",
      "Cambridge University 16\n",
      "Engineering Design Centre, University of Cambridge 13\n",
      "University of Cambridge, The Scaleup Institute, Zoopla, Founders4Schools 1\n",
      "The Psychometrics Centre, University of Cambridge 3\n",
      "Division of Biological Anthropology, University of Cambridge 8\n",
      "Cambridge Graphene Centre, University of Cambridge 1\n",
      "Univ of cambridge 2\n",
      "Dept of CST, University of Cambridge 1\n",
      "Environment and Energy Section, University of Cambridge 4\n",
      "Information Services (UIS), University of Cambridge 8\n",
      "Cambridge University Behavioural Economics Society 2\n",
      "St Edmunds College, University of Cambridge 12\n",
      "Cambridge Centre for Health Services Research (CCHSR), University of Cambridge 2\n",
      "University of Cambridge, POLIS 1\n",
      "Estate Management and Building Services, University of Cambridge 3\n",
      "SPO, Cambridge University 1\n",
      "University of Cambridge, Winton Programme 1\n",
      "Electricity Policy Research Group, University of Cambridge 1\n",
      "Centre for Business Research (CBR), University of Cambridge 10\n",
      "Museum of Archaeology & Anthropology, University of Cambridge 3\n",
      "Centre for Gender Studies, University of Cambridge 7\n",
      "Cambridge University Science and Policy Exchange (CUSPE) 29\n",
      "Centre For History & Economics, University of Cambridge 3\n",
      "Cambridge University Students on Social Policy 1\n",
      "\n",
      "HOW MANY ARE IN EACH SCHOOL\n",
      "Arts & Humanities 196\n",
      "Humanities & Social Sciences 720\n",
      "Biological Sciences 399\n",
      "Clinical Medicine 416\n",
      "Technology 621\n",
      "Independent 39\n",
      "Administration 107\n",
      "Physical Sciences 503\n",
      "\n",
      "LOCATED AND IDENTIFIED =  3001\n",
      "\n",
      "HOW MANY ARE IN EACH FACULTY\n",
      "Faculty of Architecture and History of Art 43\n",
      "Faculty of Asian and Middle Eastern Studies 9\n",
      "Faculty of Classics 16\n",
      "Faculty of Divinity 15\n",
      "Faculty of English 17\n",
      "Faculty of Modern and Medieval Languages 49\n",
      "Faculty of Music 4\n",
      "Faculty of Philosophy 19\n",
      "Centre for Research in the Arts Social Sciences and Humanities (CRASSH) 18\n",
      "Language Centre 1\n",
      "Faculty of Human, Social and Political Science (HSPS) 213\n",
      "Faculty of Economics 81\n",
      "Faculty of Education 133\n",
      "Faculty of History 47\n",
      "History and Philosophy of Science 39\n",
      "Faculty of Law 72\n",
      "Institute of Criminology 51\n",
      "Department of Land Economy 79\n",
      "Faculty of Biology 349\n",
      "Faculty of Veterinary Medicine 34\n",
      "Wellcome  MRC Cambridge Stem Cell Institute 0\n",
      "Wellcome TrustCancer Research UK Gurdon Institute 0\n",
      "Cambridge Systems Biology Centre (CSBC) 0\n",
      "MRC Toxicology Unit 0\n",
      "The Sainsbury Laboratory (SLCU) 5\n",
      "Clinical Biochemistry 6\n",
      "Clinical Neurosciences 32\n",
      "Haematology 9\n",
      "Medical Genetics 11\n",
      "Medicine 19\n",
      "MRC Biostatistics Unit 14\n",
      "MRC Cancer Unit 0\n",
      "MRC Epidemiology Unit 1\n",
      "MRC Mitochondrial Biology Unit 0\n",
      "Obstetrics & Gynaecology 3\n",
      "Oncology 17\n",
      "Paediatrics 5\n",
      "Psychiatry 56\n",
      "Public Health & Primary Care 83\n",
      "Radiology 8\n",
      "Surgery 6\n",
      "Cambridge Institute for Medical Research (CIMR) 0\n",
      "Cancer Research UK Cambridge Institute 0\n",
      "Institute of Public Health 86\n",
      "MRC Cognition and Brain Sciences Unit (MRC CBSU) 20\n",
      "Welcome Trust - MRC Institute of Metabolic Science 0\n",
      "Faculty of Engineering 343\n",
      "Faculty of Business & Management 214\n",
      "Faculty of Computer Science & Technology 0\n",
      "Department of Chemical Engineering and Biotechnology 57\n",
      "Cambridge Institute for Sustainability Leadership 0\n",
      "Cambridge University Library (CUL) 0\n",
      "Cambridge University Development and Alumni Relations (CUDAR) 0\n",
      "Fitzwilliam Museum 0\n",
      "Institute of Continuing Education (ICE) 8\n",
      "Cambridge Enterprise 25\n",
      "Careers Service 5\n",
      "Gates Trust 0\n",
      "University Information Services 1\n",
      "Development Office 20\n",
      "Finance Division 2\n",
      "Health & Safety Office 4\n",
      "Research Operations Office 5\n",
      "Press Office 1\n",
      "International Strategy Office 2\n",
      "Research Strategy Office 13\n",
      "Pro-Vice-Chancellors' Office 4\n",
      "The Vice-Chancellors Office 5\n",
      "Legal Services Office 3\n",
      "Office of External Affairs & Communications 11\n",
      "Public Engagement 1\n",
      "Personal and Professional Development 1\n",
      "Office for Translational Research 1\n",
      "Human Resources 3\n",
      "Temporary Employment Service 1\n",
      "Information Services (UIS) 0\n",
      "Academic Division 30\n",
      "Faculty of Earth Sciences & Geography 164\n",
      "Faculty of Mathematics 86\n",
      "Faculty of Physics & Chemistry 249\n",
      "Isaac Newton Institute for Mathematical Sciences 0\n"
     ]
    }
   ],
   "source": [
    "#Sorts the people from Cambridge University into departments.\n",
    "\n",
    "sorted_people = sort_people(cambridge_university, SFD)\n",
    "\n",
    "#Prints stats about the sorting procedure.\n",
    "\n",
    "print (\"PRINT HOW MANY PEOPLE ARE IN UNACCOUNTED FOR ORGANISATIONS\")\n",
    "unaccount_affiliations = [person[2] for person in sorted_people['people']]\n",
    "print ('TOTAL = ', len(sorted_people['people']))\n",
    "for affil in set(unaccount_affiliations):\n",
    "    print (affil, unaccount_affiliations.count(affil))\n",
    "\n",
    "total = 0\n",
    "    \n",
    "print (\"\")\n",
    "print (\"HOW MANY ARE IN EACH SCHOOL\")\n",
    "for school in SFD.keys():\n",
    "    print (school, sorted_people[school][\"count\"])\n",
    "    total += sorted_people[school][\"count\"]\n",
    "\n",
    "print(\"\")\n",
    "print('LOCATED AND IDENTIFIED = ', total)\n",
    "    \n",
    "print (\"\")\n",
    "print (\"HOW MANY ARE IN EACH FACULTY\")\n",
    "for school in SFD.keys():\n",
    "    for faculty in SFD[school].keys():\n",
    "        print (faculty, sorted_people[school][faculty][\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "academics = []\n",
    "\n",
    "for meeting in [[meet[3],meet[4]] for meet in meetings]:\n",
    "    if meeting not in [academic[0] for academic in academics]:\n",
    "        academics.append([meeting,[person[2] for person in people if person[0] == meeting[0] and person[1] == meeting[1]]])    \n",
    "\n",
    "cambridge_other = []\n",
    "cambridge_university = []\n",
    "colleges = []\n",
    "other_university = []\n",
    "other = []\n",
    "\n",
    "for academic in academics:\n",
    "    affiliation = [word for affiliation in academic[1] for word in perpare_organisation(affiliation)]\n",
    "    data_saved = [academic[0][0],academic[0][1],academic[1],affiliation]\n",
    "    if 'cambridge' in affiliation:\n",
    "        if 'uni' in affiliation or 'university' in affiliation or 'univ' in affiliation:\n",
    "            cambridge_university.append(data_saved)\n",
    "        elif 'college' in affiliation:\n",
    "            colleges.append(data_saved)\n",
    "        else:\n",
    "            cambridge_other.append(data_saved)\n",
    "    elif 'cambridgeshire' in affiliation:\n",
    "        if 'uni' in affiliation or 'university' in affiliation or 'univ' in affiliation:\n",
    "            cambridge_university.append(data_saved)\n",
    "        elif 'college' in affiliation:\n",
    "            colleges.append(data_saved)\n",
    "        else:\n",
    "            cambridge_other.append(data_saved)\n",
    "    elif 'college' in affiliation:\n",
    "        colleges.append(data_saved)\n",
    "    else:\n",
    "        if 'uni' in affiliation or 'university' in affiliation or 'univ' in affiliation:\n",
    "            other_university.append(data_saved)\n",
    "        else:\n",
    "            other.append(data_saved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nacademics = []\\n\\nfor meeting in [[meet[3],meet[4]] for meet in filter_meetings(meetings,datetime.strptime('2017-01-01','%Y-%m-%d'),datetime.strptime('2018-12-31','%Y-%m-%d'))]:\\n    if meeting not in [academic[0] for academic in academics]:\\n        academics.append([meeting,[person[2] for person in people if person[0] == meeting[0] and person[1] == meeting[1]]])\\n\\nfellows = []\\n\\nfor meeting in [[meet[1],meet[2]] for meet in filter_meetings(meetings,datetime.strptime('2017-01-01','%Y-%m-%d'),datetime.strptime('2018-12-31','%Y-%m-%d'))]:\\n    if meeting not in [fellow[0] for fellow in fellows]:\\n        fellows.append([meeting,[person[2] for person in people if person[0] == meeting[0] and person[1] == meeting[1]]])\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sorts a list of people into there prearranged set based upon a mapping given in the dictionary.\n",
    "#Input: List of people [first_name, second_name, primary_affilitation, ...] and a dictionary \n",
    "#'affiliation' -> [primary_tag, secondary_tag].\n",
    "#Output: List of people with tags attached [first_name, second_name, ... , primary_tag, secondary_tag]\n",
    "\n",
    "def sort_people(people, sorting_dictionary):\n",
    "    sorted_people = []\n",
    "    for person in people:\n",
    "        new_person = person.copy()\n",
    "        new_person.append(sorting_dictionary[person[2]][0])\n",
    "        new_person.append(sorting_dictionary[person[2]][1])\n",
    "        sorted_people.append()\n",
    "\n",
    "'''\n",
    "academics = []\n",
    "\n",
    "for meeting in [[meet[3],meet[4]] for meet in filter_meetings(meetings,datetime.strptime('2017-01-01','%Y-%m-%d'),datetime.strptime('2018-12-31','%Y-%m-%d'))]:\n",
    "    if meeting not in [academic[0] for academic in academics]:\n",
    "        academics.append([meeting,[person[2] for person in people if person[0] == meeting[0] and person[1] == meeting[1]]])\n",
    "\n",
    "fellows = []\n",
    "\n",
    "for meeting in [[meet[1],meet[2]] for meet in filter_meetings(meetings,datetime.strptime('2017-01-01','%Y-%m-%d'),datetime.strptime('2018-12-31','%Y-%m-%d'))]:\n",
    "    if meeting not in [fellow[0] for fellow in fellows]:\n",
    "        fellows.append([meeting,[person[2] for person in people if person[0] == meeting[0] and person[1] == meeting[1]]])\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
