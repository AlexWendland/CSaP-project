{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Alexander\\\\Desktop\\\\data\\\\GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c8f70fd09302>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mgoogle_word2vec_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\data\\GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1118\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mode should be a string'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m     \u001b[0mfobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shortcut_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Alexander\\\\Desktop\\\\data\\\\GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "#ALWAYS RUN FIRST!\n",
    "\n",
    "#Code that tries to set the largest csv field size.\n",
    "\n",
    "def max_csv_field_size():\n",
    "    maxInt = sys.maxsize\n",
    "    decrement = True\n",
    "    while decrement:\n",
    "        decrement = False\n",
    "        try:\n",
    "            csv.field_size_limit(maxInt)\n",
    "        except OverflowError:\n",
    "            maxInt = int(maxInt/10)\n",
    "            decrement = True\n",
    "\n",
    "#Import Libraries\n",
    "            \n",
    "import re\n",
    "from pylab import *\n",
    "import csv\n",
    "import sys\n",
    "max_csv_field_size()\n",
    "import psycopg2\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pickle\n",
    "import math\n",
    "import warnings\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import os\n",
    "        \n",
    "#Set the names of the files in which you want to save data.\n",
    "\n",
    "profile_data_name = 'profile_search_profile_data'\n",
    "question_data_name = 'profile_search_question_data'\n",
    "meeting_data_name = 'profile_search_meeting_data'\n",
    "event_data_name = 'profile_search_event_data'\n",
    "event_attended_data_name = 'profile_search_event_attended_data'\n",
    "\n",
    "#Set some global variables, the language of the parser and the word2vec library.\n",
    "\n",
    "parser = English()\n",
    "google_word2vec_model = KeyedVectors.load_word2vec_format(os.getcwd() + '\\data\\GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to access CRM CSaP database ...\n",
      "... data downloaded and saved to disk.\n"
     ]
    }
   ],
   "source": [
    "#Extract the data from the CRM. Don't run if you have an up to date copy of the CRM.\n",
    "\n",
    "try:\n",
    "    \n",
    "    #Opens connection to the CRM asks for peoples name and description.\n",
    "    #Output rows for row in rows row[0] - first name, row[1] - second name, row[2] - description.\n",
    "    \n",
    "    print(\"Trying to access CRM CSaP database ...\")\n",
    "    \n",
    "    conn = psycopg2.connect(SERVER_INFO)\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\"\"\"SELECT\n",
    "    person.id,\n",
    "    person.first_name,\n",
    "    person.last_name,\n",
    "    person.description\n",
    "    FROM people_person as person\n",
    "    ;\n",
    "    \"\"\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    #Saves data to the file called above.\n",
    "    \n",
    "    with open(os.getcwd() + '\\data\\\\' + profile_data_name + '.csv','w+',newline ='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        for row in rows:\n",
    "            wr.writerow([str(row[0]), row[1].encode('utf-8'), row[2].encode('utf-8'), row[3].encode('utf-8')])\n",
    "                         \n",
    "    cur.execute(\"\"\"SELECT\n",
    "    academic.id,\n",
    "    people_policyfellowshipschedule.policy_fellowship_id\n",
    "    FROM people_person as academic\n",
    "    JOIN people_policyfellowshipschedule_people ON people_policyfellowshipschedule_people.person_id = academic.id\n",
    "    JOIN people_policyfellowshipschedule ON people_policyfellowshipschedule_people.policyfellowshipschedule_id = people_policyfellowshipschedule.id\n",
    "    ;\n",
    "    \"\"\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    with open(os.getcwd() + '\\data\\\\' + meeting_data_name + '.csv','w+',newline ='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        for row in rows:\n",
    "            wr.writerow([str(row[0]), str(row[1])])\n",
    "                         \n",
    "    cur.execute(\"\"\"SELECT\n",
    "    people_policyfellowship.id,\n",
    "    fellow.first_name,\n",
    "    fellow.last_name,\n",
    "    people_policyfellowship.description,\n",
    "    question.question\n",
    "    FROM people_policyfellowship\n",
    "    LEFT JOIN people_policyfellowshipquestion as question ON people_policyfellowship.id = question.policy_fellowship_id\n",
    "    JOIN people_person as fellow ON fellow.id = people_policyfellowship.policy_fellow_id\n",
    "    ;\n",
    "    \"\"\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    #Saves data to the file called above.\n",
    "    \n",
    "    with open(os.getcwd() + '\\data\\\\' + question_data_name + '.csv','w+',newline ='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        for row in rows:\n",
    "            if row[4] is None:\n",
    "                wr.writerow([str(row[0]), row[1].encode('utf-8'), row[2].encode('utf-8'), row[3].encode('utf-8'), ''.encode('utf-8')])\n",
    "            else:\n",
    "                wr.writerow([str(row[0]), row[1].encode('utf-8'), row[2].encode('utf-8'), row[3].encode('utf-8'), row[4].encode('utf-8')])\n",
    "    \n",
    "    cur.execute(\"\"\"SELECT\n",
    "    academic.id,\n",
    "    events_personeventrole.event_id\n",
    "    FROM people_person as academic\n",
    "    JOIN events_personeventrole ON events_personeventrole.person_id = academic.id\n",
    "    ;\n",
    "    \"\"\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    #Saves data to the file called above.\n",
    "    \n",
    "    with open(os.getcwd() + '\\data\\\\' + event_attended_data_name + '.csv','w+',newline ='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        for row in rows:\n",
    "            wr.writerow([str(row[0]), str(row[1])])\n",
    "    \n",
    "    cur.execute(\"\"\"SELECT\n",
    "    event.id,\n",
    "    event.name,\n",
    "    event.description\n",
    "    FROM events_event as event\n",
    "    ;\n",
    "    \"\"\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    #Saves data to the file called above.\n",
    "    \n",
    "    with open(os.getcwd() + '\\data\\\\' + event_data_name + '.csv','w+',newline ='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        for row in rows:\n",
    "            wr.writerow([str(row[0]), row[1].encode('utf-8'), row[2].encode('utf-8')])\n",
    "    \n",
    "    print(\"... data downloaded and saved to disk.\")    \n",
    "    \n",
    "except:\n",
    "    #If server isn't online this collects data from the save file.\n",
    "    \n",
    "    print(\"... can't access server, is the tunnel set up? Can continue on previously saved data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for cleaning the text and getting it ready to be procced.\n",
    "\n",
    "#Gets rid of HTML tags and end of line markers.\n",
    "#Input: String of text from CRM or internet.\n",
    "#Output: Cleaned up string of text without HTML tags or end of line markers.\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    #Removes HTML tags.\n",
    "    \n",
    "    clean = re.compile('<.*?>')\n",
    "    temp_text = re.sub(clean, '', text)\n",
    "    \n",
    "    #Removes rouge utf-8 code.\n",
    "    \n",
    "    clean = re.compile('\\\\\\\\x\\w\\w')\n",
    "    temp_text = re.sub(clean, '', temp_text)\n",
    "    \n",
    "    clean = re.compile('\\\\\\\\x\\w')\n",
    "    temp_text = re.sub(clean, '', temp_text)\n",
    "    \n",
    "    #Removes end of line indicators and other junk.\n",
    "    \n",
    "    tags = ['\\\\r','\\\\n','/','\\\\t','\\\\']\n",
    "    \n",
    "    for tag in tags:\n",
    "        temp_text = temp_text.replace(tag,'')\n",
    "    \n",
    "    return temp_text\n",
    "\n",
    "#Tokenizes text, seperates it into a string of words and grammar.\n",
    "#Input: A string of text.\n",
    "#Output: A list of words and grammar in order all in lower case.\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "#Lemmatiser, this finds the root word (i.e. depluralises).\n",
    "#Input: a token, i.e. a single word or grammar.\n",
    "#Output: a lemma which is the base of the word and association \n",
    "\n",
    "def get_lemma(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "#Checks if two words are synonyms or antonyms.\n",
    "#Input: Takes two strings word_1 and word_2, they should be prepared words.\n",
    "#Output: Returns 1 if they are synonyms, -1 if they are antonyms or 0 otherwise.\n",
    "\n",
    "def is_synonym_antonym(word_1,word_2):\n",
    "    word_1_synonyms = set([word for synset in wn.synsets(word_1) for word in synset.lemma_names()])\n",
    "    word_2_synonyms = set([word for synset in wn.synsets(word_2) for word in synset.lemma_names()])\n",
    "    word_1_antonyms = set([ant.name() for synset in wn.synsets(word_1) for syn in synset.lemmas() for ant in syn.antonyms()])\n",
    "    word_2_antonyms = set([ant.name() for synset in wn.synsets(word_2) for syn in synset.lemmas() for ant in syn.antonyms()])\n",
    "\n",
    "    if word_1_synonyms & word_2_synonyms or word_1_antonyms & word_2_antonyms:\n",
    "        return 1\n",
    "\n",
    "    if word_1_synonyms & word_2_antonyms or word_2_synonyms & word_1_antonyms:\n",
    "        return -1\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "#Prepares text for the analysis, tokenizes texts, gets rid of words length less than 4 and filters out non-useful words then\n",
    "#Lemmatisers the text.\n",
    "#Input: A string of text you want to analysis.\n",
    "#Output: A list of Lemmas of the meaningful words.\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    \n",
    "    tokens = [tok[0] for tok in nltk.pos_tag(tokens) if tok[1][0] == 'N']\n",
    "    \n",
    "    en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    tokens = [get_lemma(token) for token in tokens if (len(token) > 4) and (token not in en_stop)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load up data ...\n",
      "... uploading people ...\n",
      "... uploading questions ...\n",
      "... uploading events ...\n",
      "... data successfully uploaded.\n"
     ]
    }
   ],
   "source": [
    "#This uploads the data into the program from the file, cleaning the data and preparing for the LDA whilst it does it.\n",
    "\n",
    "print(\"Trying to load up data ...\")\n",
    "\n",
    "people = {}\n",
    "policy_fellowships = {}\n",
    "events = {}\n",
    "\n",
    "try:\n",
    "    print('... uploading people, ...')\n",
    "    \n",
    "    with open(os.getcwd() + '\\data\\\\' + profile_data_name+'.csv', 'r') as csvfile:\n",
    "        dump = list(csv.reader(csvfile))\n",
    "        for row in dump:\n",
    "            profile = clean_text(row[3][2:-1])\n",
    "            if len(profile) > 200:\n",
    "                people[int(row[0])] = [clean_text(row[1][2:-1]), clean_text(row[2][2:-1]), prepare_text_for_lda(profile),[],[]]\n",
    "            else:\n",
    "                people[int(row[0])] = [clean_text(row[1][2:-1]), clean_text(row[2][2:-1]),'',[],[]]\n",
    "    \n",
    "    with open(os.getcwd() + '\\data\\\\' + meeting_data_name +'.csv', 'r') as csvfile:\n",
    "        dump = list(csv.reader(csvfile))\n",
    "        for row in dump:\n",
    "            people[int(row[0])][3].append(int(row[1]))\n",
    "            \n",
    "    \n",
    "    with open(os.getcwd() + '\\data\\\\' + event_attended_data_name +'.csv', 'r') as csvfile:\n",
    "        dump = list(csv.reader(csvfile))\n",
    "        for row in dump:\n",
    "            people[int(row[0])][4].append(int(row[1]))\n",
    "    \n",
    "    print('... uploading questions, ...')\n",
    "    \n",
    "    with open(os.getcwd() + '\\data\\\\' + question_data_name +'.csv', 'r') as csvfile:\n",
    "        dump = list(csv.reader(csvfile))\n",
    "        for row in dump:\n",
    "            if int(row[0]) in policy_fellowships.keys():\n",
    "                if row[4] != '':\n",
    "                    policy_fellowships[int(row[0])][2].append([clean_text(row[4][2:-1]), prepare_text_for_lda(clean_text(row[4][2:-1]))])\n",
    "            else:\n",
    "                if row[4] != '':\n",
    "                    policy_fellowships[int(row[0])] = [clean_text(row[1][2:-1]), clean_text(row[2][2:-1]), [[clean_text(row[3][2:-1]), prepare_text_for_lda(clean_text(row[3][2:-1]))],[clean_text(row[4][2:-1]), prepare_text_for_lda(clean_text(row[4][2:-1]))]]]\n",
    "                else:\n",
    "                    policy_fellowships[int(row[0])] = [clean_text(row[1][2:-1]), clean_text(row[2][2:-1]), [[clean_text(row[3][2:-1]), prepare_text_for_lda(clean_text(row[3][2:-1]))]]]\n",
    "    \n",
    "    print('... and uploading events, ...')\n",
    "    \n",
    "    with open(os.getcwd() + '\\data\\\\' + event_data_name +'.csv', 'r') as csvfile:\n",
    "        dump = list(csv.reader(csvfile))\n",
    "        for row in dump:\n",
    "            events[int(row[0])] = [clean_text(row[1][2:-1]), prepare_text_for_lda(clean_text(row[2][2:-1]))]\n",
    "    \n",
    "    print(\"... data successfully uploaded.\")\n",
    "    \n",
    "except:\n",
    "    \n",
    "    print(\".. no back up data, please connect to server.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.062*\"university\" + 0.050*\"research\" + 0.024*\"cambridge\" + 0.021*\"professor\"')\n",
      "(1, '0.067*\"health\" + 0.019*\"policy\" + 0.014*\"research\" + 0.009*\"development\"')\n",
      "(2, '0.027*\"director\" + 0.026*\"business\" + 0.017*\"management\" + 0.017*\"cambridge\"')\n",
      "(3, '0.024*\"research\" + 0.018*\"university\" + 0.013*\"cambridge\" + 0.012*\"material\"')\n",
      "(4, '0.040*\"policy\" + 0.030*\"research\" + 0.024*\"science\" + 0.019*\"university\"')\n"
     ]
    }
   ],
   "source": [
    "#Theme extractor, submit a list of texts and it will extract topic_num themes each summerised by word_num worth of words each\n",
    "#with individual ratings on how important the word is.\n",
    "#Input: List of prepared text.\n",
    "#Output: a list \n",
    "\n",
    "def get_themes(text_data, topic_num, word_num):\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "    dictionary.save('dictionary.gensim')\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = topic_num, id2word=dictionary, passes=15)\n",
    "    ldamodel.save('model5.gensim')\n",
    "    return ldamodel.print_topics(num_words=word_num)\n",
    "        \n",
    "\n",
    "#for theme in get_themes([person[3] for person in people_prepared], 5,4):\n",
    "#    print(theme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We suggest the following people from the CSaP database:\n",
      "1\\  Eden Yin as ... \n",
      "  ... their profile mensions ...\n",
      "      ... 'marketing' which is similar to 'market' that appears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'product' which is similar to 'trade' that appears in the text.\n",
      "  ... they met ...\n",
      "    ... Chris Floyd who talks about ...\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "    ... Rupert Cryer who talks about ...\n",
      "      ... 'marketing' which is similar to 'market' that appears in the text.\n",
      "      ... 'product' which is similar to 'trade' that appears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "    ... Louis Barson who talks about ...\n",
      "      ... ' trade ' which also apears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'trading' which is similar to 'trade' that appears in the text.\n",
      "      ... 'support' which is similar to 'trade' that appears in the text.\n",
      "      ... ' market ' which also apears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "    ... Karen Livingstone who talks about ...\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... ' trade ' which also apears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... ' market ' which also apears in the text.\n",
      "      ... 'marketing' which is similar to 'market' that appears in the text.\n",
      "      ... 'product' which is similar to 'trade' that appears in the text.\n",
      "    ... Rachel  King who talks about ...\n",
      "      ... ' market ' which also apears in the text.\n",
      "      ... ' trade ' which also apears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... 'support' which is similar to 'trade' that appears in the text.\n",
      "    ... Kate White who talks about ...\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... ' trade ' which also apears in the text.\n",
      "      ... 'support' which is similar to 'trade' that appears in the text.\n",
      "    ... Matthew Grainger who talks about ...\n",
      "      ... 'hand' which is similar to 'trade' that appears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "    ... Rosalind Campion who talks about ...\n",
      "      ... ' market ' which also apears in the text.\n",
      "      ... ' trade ' which also apears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "  ... they went to ...\n",
      "    ... How can behavioural insights research help reduce demand for products of illegal wildlife trade? which had in the breif ...\n",
      "      ... 'marketing' which is similar to 'market' that appears in the text.\n",
      "      ... ' trade ' which also apears in the text.\n",
      "      ... 'support' which is similar to 'trade' that appears in the text.\n",
      "2\\  Neil Gough as ... \n",
      "  ... their profile mensions ...\n",
      "      ... 'marketing' which is similar to 'market' that appears in the text.\n",
      "      ... ' market ' which also apears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "  ... they met ...\n",
      "    ... Michael Talbot who talks about ...\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'substitution' which is similar to 'trade' that appears in the text.\n",
      "      ... ' market ' which also apears in the text.\n",
      "    ... Karen Livingstone who talks about ...\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... ' trade ' which also apears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... ' market ' which also apears in the text.\n",
      "      ... 'marketing' which is similar to 'market' that appears in the text.\n",
      "      ... 'product' which is similar to 'trade' that appears in the text.\n",
      "  ... they went to ...\n",
      "    ... Dr S T Lee Public Policy Lecture: Geoff Mulgan, Nesta which had in the breif ...\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'handling' which is similar to 'trade' that appears in the text.\n",
      "    ... Dr S T Lee Public Policy Lecture 2018: Professor K Vijay Raghavan which had in the breif ...\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... 'support' which is similar to 'trade' that appears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "    ... How to stop bad data driving out the good in an age of misuse, misleadingness and misinformation which had in the breif ...\n",
      "      ... ' market ' which also apears in the text.\n",
      "    ... Christ's Climate seminar 4: Strategies for achieving serious international cooperation on climate change - theory and practice  which had in the breif ...\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... ' trade ' which also apears in the text.\n",
      "3\\  Bryony Worthington as ... \n",
      "  ... their profile mensions ...\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "  ... they met ...\n",
      "    ... David Bent who talks about ...\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... 'shift' which is similar to 'trade' that appears in the text.\n",
      "      ... 'replacement' which is similar to 'trade' that appears in the text.\n",
      "  ... they went to ...\n",
      "    ... What do the health and environment sectors in the UK have to learn from one another? which had in the breif ...\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... 'consider' which is similar to 'trade' that appears in the text.\n",
      "    ... Fostering multi-level governance in the climate change regime which had in the breif ...\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "    ... Christ's Climate seminar 4: Strategies for achieving serious international cooperation on climate change - theory and practice  which had in the breif ...\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... ' trade ' which also apears in the text.\n",
      "4\\  Charles Boulton as ... \n",
      "  ... their profile mensions ...\n",
      "      ... 'marketing' which is similar to 'market' that appears in the text.\n",
      "      ... 'product' which is similar to 'trade' that appears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... 'managing' which is similar to 'trade' that appears in the text.\n",
      "      ... 'support' which is similar to 'trade' that appears in the text.\n",
      "  ... they met ...\n",
      "    ... Chris Floyd who talks about ...\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "    ... Jonathan Clear who talks about ...\n",
      "      ... 'support' which is similar to 'trade' that appears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... ' market ' which also apears in the text.\n",
      "      ... 'manage' which is similar to 'trade' that appears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "    ... Philip Sinclair who talks about ...\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "    ... Kenan Poleo who talks about ...\n",
      "      ... ' market ' which also apears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... 'manage' which is similar to 'trade' that appears in the text.\n",
      "  ... they went to ...\n",
      "    ... CSaP Annual Conference 2012: Risk and uncertainty which had in the breif ...\n",
      "      ... ' market ' which also apears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'exchange' which is similar to 'trade' that appears in the text.\n",
      "      ... 'support' which is similar to 'trade' that appears in the text.\n",
      "5\\  Scott Collen as ... \n",
      "  ... their profile mensions ...\n",
      "      ... 'share' which is similar to 'trade' that appears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "  ... they met ...\n",
      "    ... Isidro Laso Ballesteros who talks about ...\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... 'share' which is similar to 'trade' that appears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "6\\  Eleanor Deeley as ... \n",
      "  ... their profile mensions ...\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "  ... they met ...\n",
      "    ... Mayesha Alam who talks about ...\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "7\\  Emerson Csorba as ... \n",
      "  ... their profile mensions ...\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "  ... they met ...\n",
      "    ... David Bent who talks about ...\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... 'shift' which is similar to 'trade' that appears in the text.\n",
      "      ... 'replacement' which is similar to 'trade' that appears in the text.\n",
      "8\\  Steven Broad as ... \n",
      "  ... their profile mensions ...\n",
      "      ... ' trade ' which also apears in the text.\n",
      "      ... 'support' which is similar to 'trade' that appears in the text.\n",
      "  ... they met ...\n",
      "    ... Emma Hennessey who talks about ...\n",
      "      ... ' trade ' which also apears in the text.\n",
      "      ... 'product' which is similar to 'trade' that appears in the text.\n",
      "      ... 'sharing' which is similar to 'trade' that appears in the text.\n",
      "  ... they went to ...\n",
      "    ... How can behavioural insights research help reduce demand for products of illegal wildlife trade? which had in the breif ...\n",
      "      ... 'marketing' which is similar to 'market' that appears in the text.\n",
      "      ... ' trade ' which also apears in the text.\n",
      "      ... 'support' which is similar to 'trade' that appears in the text.\n",
      "9\\  Jonathan Brech as ... \n",
      "  ... their profile mensions ...\n",
      "      ... 'marketing' which is similar to 'market' that appears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "  ... they met ...\n",
      "    ... Glenn Woodcock who talks about ...\n",
      "      ... ' market ' which also apears in the text.\n",
      "      ... 'shift' which is similar to 'trade' that appears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... 'sharing' which is similar to 'trade' that appears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "  ... they went to ...\n",
      "    ... Dr S T Lee Public Policy Lecture: Geoff Mulgan, Nesta which had in the breif ...\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'handling' which is similar to 'trade' that appears in the text.\n",
      "10\\  David Webb as ... \n",
      "  ... their profile mensions ...\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "  ... they met ...\n",
      "    ... John Ireland who talks about ...\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "    ... Glenn Woodcock who talks about ...\n",
      "      ... ' market ' which also apears in the text.\n",
      "      ... 'shift' which is similar to 'trade' that appears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... 'sharing' which is similar to 'trade' that appears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "    ... Frances Pimenta who talks about ...\n",
      "      ... ' market ' which also apears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... ' trade ' which also apears in the text.\n",
      "      ... 'exchange' which is similar to 'trade' that appears in the text.\n",
      "      ... 'sharing' which is similar to 'trade' that appears in the text.\n",
      "      ... 'product' which is similar to 'trade' that appears in the text.\n",
      "  ... they went to ...\n",
      "    ... Climate Seminars 2016: Bending the Curve on Climate which had in the breif ...\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "    ... Fostering multi-level governance in the climate change regime which had in the breif ...\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "    ... Climate seminar 2: The Climate Change Act and timescales for policy and action which had in the breif ...\n",
      "      ... ' market ' which also apears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n",
      "      ... 'business' which is similar to 'trade' that appears in the text.\n",
      "    ... Climate seminar 5: Climate change - morphing into an existential threat which had in the breif ...\n",
      "      ... ' market ' which also apears in the text.\n",
      "      ... 'change' which is similar to 'trade' that appears in the text.\n"
     ]
    }
   ],
   "source": [
    "#Checks which words where found to be similar without repeats.\n",
    "#Input: Two texts as list of strings.\n",
    "#Output: List of similar words in pairs without repeats, the first appears in text_1, the second in text_2.\n",
    "\n",
    "def get_similar_words(text_1, text_2):\n",
    "    similar_words = []\n",
    "    for word_1 in text_1:\n",
    "        for word_2 in text_2:\n",
    "            if is_synonym_antonym(word_1,word_2) != 0:\n",
    "                if [word_1, word_2] not in similar_words and [word_2, word_1] not in similar_words:\n",
    "                    similar_words.append([word_1,word_2])\n",
    "    return(similar_words)\n",
    "\n",
    "#Set words to be the words you want to compare against, this prepares this text into a nice list. (TO ADD THEME EXTRACTION)\n",
    "\n",
    "words = 'emerging markets india trade'\n",
    "\n",
    "prepared_words = prepare_text_for_lda(words)\n",
    "    \n",
    "#This searches the list of people for who are relevant to the words you have asked about and orderes them by what proportion of\n",
    "#the words in there profile fit these words.\n",
    "\n",
    "relevant_people = []\n",
    "policy_fellowships_val = {}\n",
    "events_val = {}\n",
    "\n",
    "for key in policy_fellowships.keys():\n",
    "    policy_fellowships_val[key] = 0\n",
    "    for question in policy_fellowships[key][2]:\n",
    "        for question_word in question[1]:\n",
    "            for word in prepared_words:\n",
    "                policy_fellowships_val[key] += abs(is_synonym_antonym(word,question_word))\n",
    "                \n",
    "for key in events.keys():\n",
    "    events_val[key] = 0\n",
    "    for event_word in events[key][1]:\n",
    "        for word in prepared_words:\n",
    "            events_val[key] += abs(is_synonym_antonym(word,event_word))\n",
    "\n",
    "for key in people.keys():\n",
    "    person = people[key]\n",
    "    profile_count = 0\n",
    "    policy_fellowship_count = 0\n",
    "    event_count = 0\n",
    "    if len(person[2]) > 0:\n",
    "        for word in prepared_words:\n",
    "            for profile_word in person[2]:\n",
    "                profile_count += abs(is_synonym_antonym(word,profile_word))\n",
    "        profile_count /= len(person[2])\n",
    "    if len(person[3]) > 0:\n",
    "        for policy_fellowship in person[3]:\n",
    "            policy_fellowship_count += policy_fellowships_val[policy_fellowship]\n",
    "        policy_fellowship_count /= len(person[3])\n",
    "    if len(person[4]) > 0:\n",
    "        for event in person[4]:\n",
    "            event_count += events_val[event]\n",
    "        event_count /= len(person[4])\n",
    "    \n",
    "    if profile_count > 0 or policy_fellowship_count > 0 or event_count > 0:\n",
    "        relevant_people.append([key, profile_count, policy_fellowship_count, event_count])\n",
    "        \n",
    "profile_max = max([person[1] for person in relevant_people])\n",
    "policy_fellowship_max = max([person[2] for person in relevant_people])\n",
    "event_max = max([person[3] for person in relevant_people])\n",
    "\n",
    "relevant_people = sorted(relevant_people,key=lambda person: person[1]/profile_max + person[2]/policy_fellowship_max + person[3]/event_max, reverse = True)\n",
    "\n",
    "#This prints the top 10 fitting people.\n",
    "\n",
    "print('We suggest the following people from the CSaP database:')\n",
    "\n",
    "for i in range(10):\n",
    "    person = people[relevant_people[i][0]]\n",
    "    print((str(i+1) +'\\\\ '), person[0], person[1], 'as ... ')\n",
    "    if relevant_people[i][1] > 0:\n",
    "        print('  ... their profile mensions ...' )\n",
    "        for word_pair in get_similar_words(prepared_words, person[2]):\n",
    "            if word_pair[0] == word_pair[1]:\n",
    "                print(\"      ... '\", word_pair[0], \"' which also apears in the text.\")\n",
    "            else:\n",
    "                print(\"      ... '\" + word_pair[1] + \"' which is similar to '\" + word_pair[0] + \"' that appears in the text.\")  \n",
    "    if relevant_people[i][2] > 0:\n",
    "        print('  ... they met ...')\n",
    "        for policy_fellowship_key in person[3]:\n",
    "            if policy_fellowships_val[policy_fellowship_key] != 0:\n",
    "                fellowship = policy_fellowships[policy_fellowship_key]\n",
    "                print('    ...', fellowship[0], fellowship[1], 'who talks about ...')\n",
    "                similar_words = []\n",
    "                for fellowship_text in fellowship[2]:\n",
    "                    for word_pair in get_similar_words(prepared_words,fellowship_text[1]):\n",
    "                        if word_pair not in similar_words and [word_pair[1], word_pair[0]] not in similar_words:\n",
    "                            similar_words.append(word_pair)\n",
    "                for word_pair in similar_words:\n",
    "                    if word_pair[0] == word_pair[1]:\n",
    "                        print(\"      ... '\", word_pair[0], \"' which also apears in the text.\")\n",
    "                    else:\n",
    "                        print(\"      ... '\" + word_pair[1] + \"' which is similar to '\" + word_pair[0] + \"' that appears in the text.\")  \n",
    "    if relevant_people[i][3] > 0:\n",
    "        print('  ... they went to ...')\n",
    "        for event_key in person[4]:\n",
    "            if events_val[event_key] != 0:\n",
    "                event = events[event_key]\n",
    "                print('    ...', event[0], 'which had in the breif ...')\n",
    "                for word_pair in get_similar_words(prepared_words,event[1]):\n",
    "                    if word_pair[0] == word_pair[1]:\n",
    "                        print(\"      ... '\", word_pair[0], \"' which also apears in the text.\")\n",
    "                    else:\n",
    "                        print(\"      ... '\" + word_pair[1] + \"' which is similar to '\" + word_pair[0] + \"' that appears in the text.\")       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile max: 8.939088106155396\n",
      "PF max: 12.39812058210373\n",
      "Event max: 6.110476970672607\n",
      "We suggest the following people from the CSaP database:\n",
      "1\\  Nikku Madhusudhan 0 3.9846407175064087 6.110476970672607 1.321390705237865\n",
      "2\\  Vasant Kumar 0 3.685674399137497 6.110476970672607 1.297276863435063\n",
      "3\\  Karishma Jain 0 3.3093951046466827 6.110476970672607 1.2669271590585822\n",
      "4\\  Lalita Ramakrishnan 0.40359991788864136 1.7735903561115265 6.110476970672607 1.18820317237784\n",
      "5\\  Anuj Dawar 0 1.3992066085338593 6.110476970672607 1.112856347804325\n",
      "6\\  Neil Prem 0 1.3332746028900146 6.110476970672607 1.107538444561876\n",
      "7\\  Monica Racovita 0.8295448422431946 0.0 6.110476970672607 1.0927997165249972\n",
      "8\\  Mireille El-Hajj 8.939088106155396 0 0.0 1.0\n",
      "9\\  Kate Kirk 0 0 6.110476970672607 1.0\n",
      "10\\  Oliverkhiowboong Heng 0 0 6.110476970672607 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    if relevant_people[i][1] > 0:\\n        print(\\'  ... their profile mensions ...\\' )\\n        for word_pair in get_similar_words(prepared_words, person[2]):\\n            if word_pair[0] == word_pair[1]:\\n                print(\"      ... \\'\", word_pair[0], \"\\' which also apears in the text.\")\\n            else:\\n                print(\"      ... \\'\" + word_pair[1] + \"\\' which is similar to \\'\" + word_pair[0] + \"\\' that appears in the text.\")  \\n    if relevant_people[i][2] > 0:\\n        print(\\'  ... they met ...\\')\\n        for policy_fellowship_key in person[3]:\\n            if policy_fellowships_val[policy_fellowship_key] != 0:\\n                fellowship = policy_fellowships[policy_fellowship_key]\\n                print(\\'    ...\\', fellowship[0], fellowship[1], \\'who talks about ...\\')\\n                similar_words = []\\n                for fellowship_text in fellowship[2]:\\n                    for word_pair in get_similar_words(prepared_words,fellowship_text[1]):\\n                        if word_pair not in similar_words and [word_pair[1], word_pair[0]] not in similar_words:\\n                            similar_words.append(word_pair)\\n                for word_pair in similar_words:\\n                    if word_pair[0] == word_pair[1]:\\n                        print(\"      ... \\'\", word_pair[0], \"\\' which also apears in the text.\")\\n                    else:\\n                        print(\"      ... \\'\" + word_pair[1] + \"\\' which is similar to \\'\" + word_pair[0] + \"\\' that appears in the text.\")  \\n    if relevant_people[i][3] > 0:\\n        print(\\'  ... they went to ...\\')\\n        for event_key in person[4]:\\n            if events_val[event_key] != 0:\\n                event = events[event_key]\\n                print(\\'    ...\\', event[0], \\'which had in the breif ...\\')\\n                for word_pair in get_similar_words(prepared_words,event[1]):\\n                    if word_pair[0] == word_pair[1]:\\n                        print(\"      ... \\'\", word_pair[0], \"\\' which also apears in the text.\")\\n                    else:\\n                        print(\"      ... \\'\" + word_pair[1] + \"\\' which is similar to \\'\" + word_pair[0] + \"\\' that appears in the text.\")       \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Given a dictionary of word weights returns the value of the text in sum form, no normalisation. \n",
    "#to_one = False - sums the values of each word or phrase.\n",
    "#to_one = True - checks if words from the dictionary appears if so adds the weight.\n",
    "#Input: The text to be evaluated as a list of words in order, word_dictionary with words connected to values, return_words a\n",
    "#boolean to signify whether to return the words in the output or not, and to_one to signify which counting method to use.\n",
    "\n",
    "def evaluate_text(text, word_dictionary, return_words = False, to_one = False):\n",
    "    value = 0\n",
    "    word_list = []\n",
    "        \n",
    "    for i,word in enumerate(text):\n",
    "        try:\n",
    "            value += word_dictionary[word]\n",
    "            word_list.append(word)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            value += word_dictionary[word + '_' + text[i+1]]\n",
    "            word_list.append(word + '_' + text[i+1])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            value += word_dictionary[word + '_' + text[i+1] + '_' + text[i+2]]\n",
    "            word_list.append(word + '_' + text[i+1] + '_' + text[i+2])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if to_one:\n",
    "        for word in set(word_list):\n",
    "            value -= word_dictionary[word]*(word_list.count(word)-1)\n",
    "    \n",
    "    if return_words:\n",
    "        return value, word_list\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "#Defines a dictionary where words are linked to the minimal cosin distance from a word in words, going up to the number_of_words\n",
    "#closest words in the Vector space defined by the model.\n",
    "#Input: model a Words2Vec model, words a list of words/phrases and number_of_words the number of close words you want to consider.\n",
    "#Output: A dictionary linking words with a value.\n",
    "\n",
    "def generate_word_dictionary(model, words, number_of_words = 5000):\n",
    "    word_dictionary = {}\n",
    "    for word in words:\n",
    "        word_dictionary[word] = 1\n",
    "        try:\n",
    "            for pair in word2vec_model.most_similar(positive=[word],topn=number_of_words):\n",
    "                if pair[0].lower() in word_dictionary.keys():\n",
    "                    word_dictionary[pair[0].lower()] = max([pair[1], word_dictionary[pair[0].lower()]])\n",
    "                else:\n",
    "                    word_dictionary[pair[0].lower()] = pair[1]\n",
    "        except:\n",
    "            print(word, 'not found in dictionary, no similar words discovered.')\n",
    "    \n",
    "    return word_dictionary\n",
    "\n",
    "#Set words to be the words you want to compare against, this prepares this text into a nice list. (TO ADD THEME EXTRACTION)\n",
    "\n",
    "words = ['emerging_markets']\n",
    "    \n",
    "#This searches the list of people for who are relevant to the words you have asked about and orderes them by what proportion of\n",
    "#the words in there profile fit these words.\n",
    "\n",
    "word_dictionary = generate_word_dictionary(google_word2vec_model,words)\n",
    "relevant_people = []\n",
    "policy_fellowships_val = {}\n",
    "events_val = {}\n",
    "\n",
    "for key in policy_fellowships.keys():\n",
    "    policy_fellowships_val[key] = 0\n",
    "    for question in policy_fellowships[key][2]:\n",
    "        policy_fellowships_val[key] += evaluate_text(question[1],word_dictionary)\n",
    "                \n",
    "for key in events.keys():\n",
    "    events_val[key] = evaluate_text(events[key][1],word_dictionary)\n",
    "\n",
    "for key in people.keys():\n",
    "    person = people[key]\n",
    "    profile_count = 0\n",
    "    policy_fellowship_count = 0\n",
    "    event_count = 0\n",
    "    if len(person[2]) > 0:\n",
    "        profile_count = evaluate_text(person[2],word_dictionary)\n",
    "    if len(person[3]) > 0:\n",
    "        for policy_fellowship in person[3]:\n",
    "            policy_fellowship_count += policy_fellowships_val[policy_fellowship]\n",
    "        policy_fellowship_count /= len(person[3])\n",
    "    if len(person[4]) > 0:\n",
    "        for event in person[4]:\n",
    "            event_count += events_val[event]\n",
    "        event_count /= len(person[4])\n",
    "    \n",
    "    if profile_count > 0 or policy_fellowship_count > 0 or event_count > 0:\n",
    "        relevant_people.append([key, profile_count, policy_fellowship_count, event_count])\n",
    "        \n",
    "profile_max = max([person[1] for person in relevant_people])\n",
    "policy_fellowship_max = max([person[2] for person in relevant_people])\n",
    "event_max = max([person[3] for person in relevant_people])\n",
    "\n",
    "print('Profile max:',profile_max)\n",
    "print('PF max:', policy_fellowship_max)\n",
    "print('Event max:', event_max)\n",
    "\n",
    "relevant_people = sorted(relevant_people,key=lambda person: person[1]/profile_max + person[2]/policy_fellowship_max + person[3]/event_max, reverse = True)\n",
    "\n",
    "#This prints the top 10 fitting people.\n",
    "\n",
    "print('We suggest the following people from the CSaP database:')\n",
    "\n",
    "for i in range(10):\n",
    "    person = people[relevant_people[i][0]]\n",
    "    print((str(i+1) +'\\\\ '), person[0], person[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
